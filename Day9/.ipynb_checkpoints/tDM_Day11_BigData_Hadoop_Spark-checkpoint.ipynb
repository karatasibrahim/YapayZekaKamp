{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PbujUmKSgTcE"
   },
   "source": [
    "# <center>What is SPARK</center>\n",
    "\n",
    "Apache®Spark™is a powerful open source processing engine built around speed, ease of use, and sophisticated analytics. It was originally developed at UC Berkeley in 2009.\n",
    "\n",
    "Spark runs on Hadoop, Mesos, standalone,  or in the cloud. It can access diverse data sources including HDFS, Cassandra,  HBase, and S3.\n",
    "\n",
    "Company Name : theDataScience Bootcamp <br>\n",
    "Prepared by: Zafer Acar <br>\n",
    "\n",
    "MDS-DL Document Version : 1.0 <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lMHtK8MwgTcM"
   },
   "source": [
    "## What are the benefits of Apache Spark?\n",
    "### Speed \n",
    "Engineered from the bottom-up for performance, Spark can be100x faster than Hadoop for large scale data processing\n",
    "\n",
    "### Ease of Use\n",
    "Spark has easy-to-use APIs for operating on large datasets. \n",
    "\n",
    "### A Unified Engine\n",
    "Spark comes packaged with higher-level libraries, including support for SQL queries, streaming data, machine learning and graph processing. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tr8-LzdUgTcN"
   },
   "source": [
    "## Before we begin\n",
    "install java jdk 8. New versions of java do not SUPPORT spark. Remove upper versions and install java jdk 8 "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "pqrmgY7jgTcO"
   },
   "source": [
    "!pip install spark"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "fJgZDYUvgTcQ"
   },
   "source": [
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gJAWayZ4gTcQ"
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext \n",
    "import collections\n",
    "\n",
    "sc = SparkContext()\n",
    "rdd=sc.parallelize([4,5,2,2])\n",
    "\n",
    "sq=rdd.map(lambda x:x*x)\n",
    "\n",
    "print(sq.collect())\n",
    "\n",
    "\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "roMwyDDAgTcS"
   },
   "outputs": [],
   "source": [
    "sc.stop() # closes the session, without closing the seesion, we can't run another session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LaHW4q2dgTcS"
   },
   "source": [
    "## Ratings Histogram\n",
    "We'll run a simple Spark script using Python and analyze the 100,000 movie ratings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kE39fH8LgTcT"
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext \n",
    "import collections\n",
    "\n",
    "conf = SparkConf().setMaster(\"local\").setAppName(\"RatingsHistogram\")\n",
    "sc = SparkContext(conf = conf)\n",
    "\n",
    "lines = sc.textFile(\"spark/ml-100k/u.data\")\n",
    "ratings = lines.map(lambda x: x.split()[2])\n",
    "result = ratings.countByValue()\n",
    "sortedResults = collections.OrderedDict(sorted(result.items()))\n",
    "for key, value in sortedResults.items():\n",
    "    print(\"%s %i\" % (key, value))\n",
    "\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CHibidjUgTcV"
   },
   "source": [
    "## the Average Friends by Age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4Vl628legTcV"
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "\n",
    "conf = SparkConf().setMaster(\"local\").setAppName(\"FriendsByAge\")\n",
    "sc = SparkContext(conf = conf)\n",
    "\n",
    "def parseLine(line):\n",
    "    fields = line.split(',')\n",
    "    age = int(fields[2])\n",
    "    numFriends = int(fields[3])\n",
    "    return (age, numFriends)\n",
    "\n",
    "lines = sc.textFile(\"spark/fakefriends.csv\")\n",
    "rdd = lines.map(parseLine)\n",
    "totalsByAge = rdd.mapValues(lambda x: (x, 1)).reduceByKey(lambda x, y: (x[0] + y[0], x[1] + y[1]))\n",
    "averagesByAge = totalsByAge.mapValues(lambda x: int(x[0] / x[1]))\n",
    "results = averagesByAge.collect()\n",
    "for result in results:\n",
    "    print(result)\n",
    "\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pI3hAfwEgTcW"
   },
   "source": [
    "## Filtering RDDs and the Minimum Temperature by Location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "spE0ElbIgTcX"
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "\n",
    "conf = SparkConf().setMaster(\"local\").setAppName(\"MinTemperatures\")\n",
    "sc = SparkContext(conf = conf)\n",
    "\n",
    "def parseLine(line):\n",
    "    fields = line.split(',')\n",
    "    stationID = fields[0]\n",
    "    entryType = fields[2]\n",
    "    temperature = float(fields[3]) * 0.1 * (9.0 / 5.0) + 32.0\n",
    "    return (stationID, entryType, temperature)\n",
    "\n",
    "lines = sc.textFile(\"spark/1800.csv\")\n",
    "parsedLines = lines.map(parseLine)\n",
    "minTemps = parsedLines.filter(lambda x: \"TMIN\" in x[1])\n",
    "stationTemps = minTemps.map(lambda x: (x[0], x[2]))\n",
    "minTemps = stationTemps.reduceByKey(lambda x, y: min(x,y))\n",
    "results = minTemps.collect();\n",
    "\n",
    "for result in results:\n",
    "    print(result[0] + \"\\t{:.2f}F\".format(result[1]))\n",
    "    \n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8C7y64NdgTcY"
   },
   "source": [
    "## the Maximum Temperature by Location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y5nQg20RgTcY"
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "\n",
    "conf = SparkConf().setMaster(\"local\").setAppName(\"MaxTemperatures\")\n",
    "sc = SparkContext(conf = conf)\n",
    "\n",
    "def parseLine(line):\n",
    "    fields = line.split(',')\n",
    "    stationID = fields[0]\n",
    "    entryType = fields[2]\n",
    "    temperature = float(fields[3]) * 0.1 * (9.0 / 5.0) + 32.0\n",
    "    return (stationID, entryType, temperature)\n",
    "\n",
    "lines = sc.textFile(\"spark/1800.csv\")\n",
    "parsedLines = lines.map(parseLine)\n",
    "maxTemps = parsedLines.filter(lambda x: \"TMAX\" in x[1])\n",
    "stationTemps = maxTemps.map(lambda x: (x[0], x[2]))\n",
    "maxTemps = stationTemps.reduceByKey(lambda x, y: max(x,y))\n",
    "results = maxTemps.collect();\n",
    "\n",
    "for result in results:\n",
    "    print(result[0] + \"\\t{:.2f}F\".format(result[1]))\n",
    "\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9zzwGY83gTcZ"
   },
   "source": [
    "## Counting Word Occurrences Using flatmap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bb5LCxRygTcZ"
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "\n",
    "conf = SparkConf().setMaster(\"local\").setAppName(\"WordCount\")\n",
    "sc = SparkContext(conf = conf)\n",
    "\n",
    "input = sc.textFile(\"spark/book.txt\")\n",
    "words = input.flatMap(lambda x: x.split())\n",
    "wordCounts = words.countByValue()\n",
    "\n",
    "for word, count in wordCounts.items():\n",
    "    cleanWord = word.encode('ascii', 'ignore')\n",
    "    if (cleanWord):\n",
    "        print(cleanWord.decode() + \" \" + str(count))\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XKgoxegkgTca"
   },
   "source": [
    "## Improving the Word Count Script with Regular Expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5XFAYNf7gTca"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from pyspark import SparkConf, SparkContext\n",
    "\n",
    "def normalizeWords(text):\n",
    "    return re.compile(r'\\W+', re.UNICODE).split(text.lower())\n",
    "\n",
    "conf = SparkConf().setMaster(\"local\").setAppName(\"WordCount\")\n",
    "sc = SparkContext(conf = conf)\n",
    "\n",
    "input = sc.textFile(\"spark/book.txt\")\n",
    "words = input.flatMap(normalizeWords)\n",
    "wordCounts = words.countByValue()\n",
    "\n",
    "for word, count in wordCounts.items():\n",
    "    cleanWord = word.encode('ascii', 'ignore')\n",
    "    if (cleanWord):\n",
    "        print(cleanWord.decode() + \" \" + str(count))\n",
    "\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ETd098cygTca"
   },
   "source": [
    "## Sorting the Word Count Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M-1qONzLgTcb"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from pyspark import SparkConf, SparkContext\n",
    "\n",
    "def normalizeWords(text):\n",
    "    return re.compile(r'\\W+', re.UNICODE).split(text.lower())\n",
    "\n",
    "conf = SparkConf().setMaster(\"local\").setAppName(\"WordCount\")\n",
    "sc = SparkContext(conf = conf)\n",
    "\n",
    "input = sc.textFile(\"spark/book.txt\")\n",
    "words = input.flatMap(normalizeWords)\n",
    "\n",
    "wordCounts = words.map(lambda x: (x, 1)).reduceByKey(lambda x, y: x + y)\n",
    "wordCountsSorted = wordCounts.map(lambda x: (x[1], x[0])).sortByKey(False)\n",
    "results = wordCountsSorted.collect()\n",
    "\n",
    "for result in results:\n",
    "    count = str(result[0])\n",
    "    word = result[1].encode('ascii', 'ignore')\n",
    "    if (word):\n",
    "        print(word.decode() + \":\\t\\t\" + count)\n",
    "\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aQL5xtYygTcc"
   },
   "source": [
    "## Find the Total Amount Spent by Customer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XpZMT2cmgTcc"
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "\n",
    "conf = SparkConf().setMaster(\"local\").setAppName(\"SpendByCustomer\")\n",
    "sc = SparkContext(conf = conf)\n",
    "\n",
    "def extractCustomerPricePairs(line):\n",
    "    fields = line.split(',')\n",
    "    return (int(fields[0]), float(fields[2]))\n",
    "\n",
    "input = sc.textFile(\"spark/customer-orders.csv\")\n",
    "mappedInput = input.map(extractCustomerPricePairs)\n",
    "totalByCustomer = mappedInput.reduceByKey(lambda x, y: x + y)\n",
    "\n",
    "results = totalByCustomer.collect();\n",
    "for result in results:\n",
    "    print(result)\n",
    "\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Gyx33l0gTcc"
   },
   "source": [
    "## Check Your Results and Sort Them by Total Amount Spent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5L9In3YggTcd"
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "\n",
    "conf = SparkConf().setMaster(\"local\").setAppName(\"SpendByCustomerSorted\")\n",
    "sc = SparkContext(conf = conf)\n",
    "\n",
    "def extractCustomerPricePairs(line):\n",
    "    fields = line.split(',')\n",
    "    return (int(fields[0]), float(fields[2]))\n",
    "\n",
    "input = sc.textFile(\"spark/customer-orders.csv\")\n",
    "mappedInput = input.map(extractCustomerPricePairs)\n",
    "totalByCustomer = mappedInput.reduceByKey(lambda x, y: x + y)\n",
    "\n",
    "#Changed for Python 3 compatibility:\n",
    "#flipped = totalByCustomer.map(lambda (x,y):(y,x))\n",
    "flipped = totalByCustomer.map(lambda x: (x[1], x[0]))\n",
    "\n",
    "totalByCustomerSorted = flipped.sortByKey()\n",
    "\n",
    "results = totalByCustomerSorted.collect();\n",
    "for result in results:\n",
    "    print(result)\n",
    "\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kVr8d3O_gTcd"
   },
   "source": [
    "## Find the most rated movie in the MovieLens dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hzklUDEFgTcd"
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "\n",
    "conf = SparkConf().setMaster(\"local\").setAppName(\"PopularMovies\")\n",
    "sc = SparkContext(conf = conf)\n",
    "\n",
    "lines = sc.textFile(\"spark/ml-100k/u.data\")\n",
    "movies = lines.map(lambda x: (int(x.split()[1]), 1))\n",
    "movieCounts = movies.reduceByKey(lambda x, y: x + y)\n",
    "\n",
    "flipped = movieCounts.map( lambda xy : (xy[1], xy[0]) )\n",
    "sortedMovies = flipped.sortByKey(False) #(False) Descending order\n",
    "\n",
    "results = sortedMovies.collect()\n",
    "\n",
    "for result in results:\n",
    "    print(result)\n",
    "\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RtWSZof6gTce"
   },
   "source": [
    "## Use Broadcast Variables to Display Movie Names Instead of ID Numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pi4x88KKgTce"
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "\n",
    "def loadMovieNames():\n",
    "    movieNames = {}\n",
    "    with open(\"spark/ml-100k/u.item\", encoding = \"ISO-8859-1\") as f:\n",
    "        for line in f:\n",
    "            fields = line.split('|')\n",
    "            movieNames[int(fields[0])] = fields[1]\n",
    "    return movieNames\n",
    "\n",
    "conf = SparkConf().setMaster(\"local\").setAppName(\"PopularMovies\")\n",
    "sc = SparkContext(conf = conf)\n",
    "\n",
    "nameDict = sc.broadcast(loadMovieNames())\n",
    "\n",
    "lines = sc.textFile(\"spark/ml-100k/u.data\")\n",
    "movies = lines.map(lambda x: (int(x.split()[1]), 1))\n",
    "movieCounts = movies.reduceByKey(lambda x, y: x + y)\n",
    "\n",
    "flipped = movieCounts.map(lambda xy : (xy[1], xy[0]))\n",
    "sortedMovies = flipped.sortByKey(False)\n",
    "\n",
    "sortedMoviesWithNames = sortedMovies.map(lambda countMovie : (nameDict.value[countMovie[1]], countMovie[0]))\n",
    "\n",
    "results = sortedMoviesWithNames.collect()\n",
    "\n",
    "for result in results:\n",
    "    print(result)\n",
    "    \n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F4nkpoiTgTce"
   },
   "source": [
    "## Find the Most Popular Superhero in a Social Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4HpzPrwngTcf"
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "\n",
    "conf = SparkConf().setMaster(\"local\").setAppName(\"PopularHero\")\n",
    "sc = SparkContext(conf = conf)\n",
    "\n",
    "def countCoOccurences(line):\n",
    "    elements = line.split()\n",
    "    return (int(elements[0]), len(elements) - 1)\n",
    "\n",
    "def parseNames(line):\n",
    "    fields = line.split('\\\"')\n",
    "    return (int(fields[0]), fields[1].encode(\"utf8\"))\n",
    "\n",
    "names = sc.textFile(\"spark/Marvel-names.txt\")\n",
    "namesRdd = names.map(parseNames)\n",
    "\n",
    "lines = sc.textFile(\"spark/Marvel-graph.txt\")\n",
    "\n",
    "pairings = lines.map(countCoOccurences)\n",
    "totalFriendsByCharacter = pairings.reduceByKey(lambda x, y : x + y)\n",
    "flipped = totalFriendsByCharacter.map(lambda xy : (xy[1], xy[0]))\n",
    "\n",
    "mostPopular = flipped.max()\n",
    "\n",
    "mostPopularName = namesRdd.lookup(mostPopular[1])[0]\n",
    "\n",
    "print(str(mostPopularName) + \" is the most popular superhero, with \" + str(mostPopular[0]) + \\\n",
    "      \" co-appearances.\")\n",
    "\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2-sCX-rkgTcf"
   },
   "source": [
    "# Introducing SparkSQL\n",
    "We'll cover the concepts of SparkSQL, DataFrames, and DataSets, and why they are so important in Spark 2.0 and above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gq5ZJ5HkgTcf"
   },
   "source": [
    "## A real example, revisiting our fake social network data and analyzing it with DataFrames through a SparkSession object\n",
    "\n",
    "Executing SQL Commands and SQL-Style Functions on a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "55TJsxJlgTcf"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "\n",
    "import collections\n",
    "\n",
    "# Create a SparkSession (Note, the config section is only for Windows!)\n",
    "spark = SparkSession.builder.config(\"spark.sql.warehouse.dir\", \"file:///C:/temp\").appName(\"SparkSQL\").getOrCreate()\n",
    "\n",
    "def mapper(line):\n",
    "    fields = line.split(',')\n",
    "    return Row(ID=int(fields[0]), name=str(fields[1].encode(\"utf-8\")), age=int(fields[2]), numFriends=int(fields[3]))\n",
    "\n",
    "lines = spark.sparkContext.textFile(\"spark/fakefriends.csv\")\n",
    "people = lines.map(mapper)\n",
    "\n",
    "# Infer the schema, and register the DataFrame as a table.\n",
    "schemaPeople = spark.createDataFrame(people).cache()\n",
    "schemaPeople.createOrReplaceTempView(\"people\")\n",
    "\n",
    "# SQL can be run over DataFrames that have been registered as a table.\n",
    "teenagers = spark.sql(\"SELECT * FROM people WHERE age >= 13 AND age <= 19\")\n",
    "\n",
    "# The results of SQL queries are RDDs and support all the normal RDD operations.\n",
    "for teen in teenagers.collect():\n",
    "  print(teen)\n",
    "\n",
    "# We can also use functions instead of SQL queries:\n",
    "schemaPeople.groupBy(\"age\").count().orderBy(\"age\").show()\n",
    "\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Wg59PB8gTcg"
   },
   "source": [
    "## Using DataFrames Instead of RDDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FhZ2sbN0gTcg"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql import functions\n",
    "\n",
    "def loadMovieNames():\n",
    "    movieNames = {}\n",
    "    with open(\"spark/ml-100k/u.item\") as f:\n",
    "        for line in f:\n",
    "            fields = line.split('|')\n",
    "            movieNames[int(fields[0])] = fields[1]\n",
    "    return movieNames\n",
    "\n",
    "# Create a SparkSession (the config bit is only for Windows!)\n",
    "spark = SparkSession.builder.config(\"spark.sql.warehouse.dir\", \"file:///C:/temp\").appName(\"PopularMovies\").getOrCreate()\n",
    "\n",
    "# Load up our movie ID -> name dictionary\n",
    "nameDict = loadMovieNames()\n",
    "\n",
    "# Get the raw data\n",
    "lines = spark.sparkContext.textFile(\"spark/ml-100k/u.data\")\n",
    "# Convert it to a RDD of Row objects\n",
    "movies = lines.map(lambda x: Row(movieID =int(x.split()[1])))\n",
    "# Convert that to a DataFrame\n",
    "movieDataset = spark.createDataFrame(movies)\n",
    "\n",
    "# Some SQL-style magic to sort all movies by popularity in one line!\n",
    "topMovieIDs = movieDataset.groupBy(\"movieID\").count().orderBy(\"count\", ascending=False).cache()\n",
    "\n",
    "# Show the results at this point:\n",
    "\n",
    "#|movieID|count|\n",
    "#+-------+-----+\n",
    "#|     50|  584|\n",
    "#|    258|  509|\n",
    "#|    100|  508|\n",
    "\n",
    "topMovieIDs.show()\n",
    "\n",
    "# Grab the top 10\n",
    "top10 = topMovieIDs.take(10)\n",
    "\n",
    "# Print the results\n",
    "print(\"\\n\")\n",
    "for result in top10:\n",
    "    # Each row has movieID, count as above.\n",
    "    print(\"%s: %d\" % (nameDict[result[0]], result[1]))\n",
    "\n",
    "# Stop the session\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X3HyWprEgTch"
   },
   "source": [
    "## Introducing MLLIB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7ASbofWWgTch"
   },
   "source": [
    "We'll briefly cover the capabilities of Spark's MLLib machine learning library, and how it can help you solve data mining, machine learning, and statistical problems you may encounter. We'll go into more depth on MLLib's Alternating Least Squares (ALS) recommendation engine, and how we can use it to produce movie recommendations with the MovieLens dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gJZnnUZQgTch",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.mllib.recommendation import ALS, Rating\n",
    "\n",
    "def loadMovieNames():\n",
    "    movieNames = {}\n",
    "    with open(\"spark/ml-100k/u.item\") as f:\n",
    "        for line in f:\n",
    "            fields = line.split('|')\n",
    "            movieNames[int(fields[0])] = fields[1] #.decode('ascii', 'ignore')\n",
    "    return movieNames\n",
    "\n",
    "conf = SparkConf().setMaster(\"local[*]\").setAppName(\"MovieRecommendationsALS\")\n",
    "sc = SparkContext(conf = conf)\n",
    "sc.setCheckpointDir('checkpoint')\n",
    "\n",
    "print(\"\\nLoading movie names...\")\n",
    "nameDict = loadMovieNames()\n",
    "\n",
    "data = sc.textFile(\"spark/ml-100k/u.data\")\n",
    "\n",
    "ratings = data.map(lambda l: l.split()).map(lambda l: Rating(int(l[0]), int(l[1]), float(l[2]))).cache()\n",
    "\n",
    "# Build the recommendation model using Alternating Least Squares\n",
    "print(\"\\nTraining recommendation model...\")\n",
    "rank = 10\n",
    "# Lowered numIterations to ensure it works on lower-end systems\n",
    "numIterations = 6\n",
    "model = ALS.train(ratings, rank, numIterations)\n",
    "\n",
    "userID = int(sys.argv[1])\n",
    "\n",
    "print(\"\\nRatings for user ID \" + str(userID) + \":\")\n",
    "userRatings = ratings.filter(lambda l: l[0] ==0)\n",
    "for rating in userRatings.collect():\n",
    "    print (nameDict[int(rating[1])] + \": \" + str(rating[2]))\n",
    "\n",
    "print(\"\\nTop 10 recommendations:\")\n",
    "recommendations = model.recommendProducts(userID, 10)\n",
    "for recommendation in recommendations:\n",
    "    print (nameDict[int(recommendation[1])] + \" score \" + str(recommendation[2]))\n",
    "\n",
    "\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "62NqIl6hgTci"
   },
   "source": [
    "# Linear Regression with Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EWdyjjgagTci"
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "# Create a SparkSession (Note, the config section is only for Windows!)\n",
    "spark = SparkSession.builder.config(\"spark.sql.warehouse.dir\", \"file:///C:/temp\").appName(\"LinearRegression\").getOrCreate()\n",
    "\n",
    "# Load up our data and convert it to the format MLLib expects.\n",
    "inputLines = spark.sparkContext.textFile(\"spark/regression.txt\")\n",
    "data = inputLines.map(lambda x: x.split(\",\")).map(lambda x: (float(x[0]), Vectors.dense(float(x[1]))))\n",
    "\n",
    "# Convert this RDD to a DataFrame\n",
    "colNames = [\"label\", \"features\"]\n",
    "df = data.toDF(colNames)\n",
    "\n",
    "# Note, there are lots of cases where you can avoid going from an RDD to a DataFrame.\n",
    "# Perhaps you're importing data from a real database. Or you are using structured streaming\n",
    "# to get your data.\n",
    "\n",
    "# Let's split our data into training data and testing data\n",
    "trainTest = df.randomSplit([0.5, 0.5])\n",
    "trainingDF = trainTest[0]\n",
    "testDF = trainTest[1]\n",
    "\n",
    "# Now create our linear regression model\n",
    "lir = LinearRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8)\n",
    "\n",
    "# Train the model using our training data\n",
    "model = lir.fit(trainingDF)\n",
    "\n",
    "# Now see if we can predict values in our test data.\n",
    "# Generate predictions using our linear regression model for all features in our\n",
    "# test dataframe:\n",
    "fullPredictions = model.transform(testDF).cache()\n",
    "\n",
    "# Extract the predictions and the \"known\" correct labels.\n",
    "predictions = fullPredictions.select(\"prediction\").rdd.map(lambda x: x[0])\n",
    "labels = fullPredictions.select(\"label\").rdd.map(lambda x: x[0])\n",
    "\n",
    "# Zip them together\n",
    "predictionAndLabel = predictions.zip(labels).collect()\n",
    "\n",
    "# Print out the predicted and actual values for each point\n",
    "for prediction in predictionAndLabel:\n",
    "  print(prediction)\n",
    "\n",
    "\n",
    "# Stop the session\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P63s_th9gTcj"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
